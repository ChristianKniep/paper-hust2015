\subsubsection{Log Pipeline}
In the past Log engines like splunk were high priced data consumers, which proposed to help put with the
flowd of incomming unstructured data. Without such framework is was very hard to normalize all the incomming data in a sane way.

In the last couple of years a couple of open-source variants of such tools emerged. The mst prominent is Logstash, followed by Graylog.
They provide a modular pipeline, which allows to take data from different inputs, normalize it (common sense in some JSON-style format)
and alter the information on the way through. Among others the data is then persisted in a datastore (common sense backend is Elasticsearch).



\begin{lstlisting}[language=bash,
    caption={Basic Logstash configuration},
    label={lst:ls_cfg}]
input {
    # consuming message send via SYSLOG
    syslog {
        port => 5514
        type => syslog
    }
    # output of the INPUT is a json blob
}
filter {
    mutate {
        # add tags to JSON blob
        add_tag => [ "special" ]
    }
}

output {
    # output the event on stdout
    stdout { codec => rubydebug }
}
\end{lstlisting}

The power of these pipelines lies in the flexibility they provide. With a couple of lines of configuration the logs are centralised and accessible
via a intuitive dashboard like never before. Compared to plain text files on each server itself this changes the way logs are correlated with each other.

