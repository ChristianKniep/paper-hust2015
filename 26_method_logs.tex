\subsubsection{Log Pipeline}
In the past Log engines like \gls{splunk} (\glsdesc{splunk}) were high priced data consumers, which proposed to help deal with the
flow of incoming unstructured data. Without such framework is was very hard to normalize all the data to make use of it.

In the last couple of years a couple of open-source variants of such tools emerged. The most prominent is \gls{logstash}, followed by \gls{graylog} (\glsdesc{graylog}).
They provide a modular pipeline, which allows to take data from different inputs, normalize it (common sense in some JSON-style format)
and alter the information on the way through. Among others the data is then persisted in a data-store, e.g. in \gls{elasticsearch} (\glsdesc{elasticsearch}).
Both ecosystems provide visualization tools and connectors to a rich ecosystem.

\begin{lstlisting}[language=bash,label={lst:ls_cfg},
    caption={Basic Logstash configuration}]

input {
    syslog {
        port => 5514
        type => syslog
    }
}
filter {
    mutate {
        add_tag => [ "special" ]
    }
}

output {
    stdout { codec => rubydebug }
}
\end{lstlisting}

The power of these pipelines lies in the flexibility they provide. With a couple of lines of configuration the logs are centralized and accessible
via a intuitive dashboard like never before. Compared to plain text files on each server itself this changes the way logs are correlated with each other.

